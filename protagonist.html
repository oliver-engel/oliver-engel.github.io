<!DOCTYPE html>

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Primary Meta Tags -->
  <title>Oliver Engel Design Portfolio</title>
  <meta name="title" content="Oliver Engel Design Portfolio">
  <meta name="description" content="Human-Computer Interaction + Design graduate student at the University of Washington, Seattle.">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-42025560-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'UA-42025560-2');
  </script>

  <meta name="robots" content="index, follow">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content=" Design portfolio of Oliver Engel | HCI & Design graduate student at the University of Washington, Seattle " />
  <meta name="keywords" content="oliver engel portfolio, portfolio design, portfolio oliverengel, minimal portfolio, photography, graphic design " />
  <meta name="author" content="Oliver Engel">
  <meta name="robots" content="NOODP">

  <!-- STYLESHEETS -->
  <link rel="stylesheet" href="css/main.css">
  <link rel="stylesheet" href="css/nav.css">
  <link rel="stylesheet" href="css/index.css">
  <link rel="stylesheet" href="css/project.css">
  <link rel="stylesheet" href="css/animations.css">
  <!-- <link rel="stylesheet" href="css/microsoft-exp.css"> -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
  <link href="https://fonts.googleapis.com/css?family=Lora:400,700" rel="stylesheet">

  <link rel="icon" type="image/png" sizes="32x32" href="favicons/favicon-32x32.png?v=kP3m5Xdgdx">
  <link rel="icon" type="image/png" sizes="16x16" href="favicons/favicon-16x16.png?v=kP3m5Xdgdx">
  <link rel="manifest" href="favicons/site.webmanifest?v=kP3m5Xdgdx">
  <link rel="shortcut icon" href="favicons/favicon.ico?v=kP3m5Xdgdx">
  <meta name="theme-color" content="#ffffff">

</head>

<body>

  <svg id="fader"></svg>

  <nav class="nav" id="hide-nav">
    <div class="nav-container">
      <div class="links">
        <a class="sibling-fade" href="index.html" style="color:var(--nav-links)">üè† Work</a>
        <a class="sibling-fade" href="writing.html" style="color:var(--nav-links)">‚úèÔ∏è Writing</a>
        <a class="sibling-fade" href="about.html" style="color:var(--nav-links)">ü•ù About</a>
      </div>
    </div>
  </nav>

  <aside class="u-marginAuto u-maxWidth1032 js-postLeftSidebar ">
    <div id="menu" class=" hide-menu u-foreground u-top0 u-transition--fadeOut300 u-fixed u-sm-hide" data-scroll="fixed" style="transform: translateY(150px);">
      <ul id="side-list">
        <a href="#overview">
          <li>Overview</li>
        </a>
        <a href="#ideation">
          <li>Ideation</li>
        </a>
        <a href="#prototyping">
          <li>Prototyping</li>
        </a>
        <a href="#interaction">
          <li>Interaction Design</li>
        </a>
        <a href="#keyfeatures">
          <li>Key Features</li>
        </a>
        <a href="#reflection">
          <li>Reflection</li>
        </a>
      </ul>
    </div>
  </aside>


  <div class="project-header fade-in big-image" style="z-index:4; background-color:#ad8800">
    <div class="project-header-content rellax fade-scroll" data-rellax-speed="-2">
      <h2>‚ÄúProtagonist‚Äù is an immersive choose-your-own-adventure narrative experience where a user assumes the lead role in an audio-only story.</h2>
    </div>

    <div style="text-align:center; position:absolute; color:white; bottom: 10vh;" class="CTA">
      Listen closely...
      <div class="scroll-down"></div>
    </div>
  </div>


  <div class="page-content">
    <div class="container">

      <img class="big-image image-crop-headers has-animation" src="img/protagonist/hero.jpg">

      <div class="grid-1 " style="width:100%;">

        <div class="project-info">
          <div>
            <p class="text-uppercase">ROLE</p>
            <p>Designer<br>Technical Writer</p>
          </div>
          <div>
            <p class="text-uppercase">LENGTH</p>
            <p>3 weeks</p>
          </div>
          <div class="mobile-remove">
            <p class="text-uppercase">YEAR</p>
            <p>2019</p>
          </div>
        </div>

      </div>



      <div class="grid-1 has-animation">
        <h3 id="overview">Overview</h3>
        <p><span style="font-weight:900; text-transform:uppercase">Protagonist</span> is a voice-controlled experience for the Google Home that responds to the growing segment of audio-only interactive media. It plays to the strengths of audio recognition and analysis to transform passive listening into immersive engagement.</p>
        <p>When prompted, users use their voice to respond to other characters in a story and subsequently affect the narrative arc, depending on how they respond. Think of it like an extension of <a href="https://goosebumps.scholastic.com/books" target="_blank">Goosebumps</a>, or more recently, <a href="https://en.wikipedia.org/wiki/Black_Mirror:_Bandersnatch" target="_blank">Bandersnatch</a>.</p>
        <p>Users can give open-ended responses, which are analyzed for content and emotional qualities to choose which narrative path should be taken. This differs from existing choose-your-own-adventure media in that the user is not restricted to selecting from a predefined list of options, but rather can be expressive in their response and let the algorithm sort out the rest.</p>
        <blockquote>* Please note that this is a technical specification write-up! It's more meant for the eyes of developers, who would be making the product. However, I believe it showcases the level of detail I use to document projects.</blockquote>
      </div>

      <div class="has-animation">
        <img style="margin-top:15px; width:100%" data-src="img/protagonist/start.jpg" alt="Experiential prototyping with kids" data-action="zoom" class="lazy big-image">
        <p class="caption">Fig 1. Triggering a story via the app entry point.</p>
      </div>

      <div class="grid-1 has-animation ">
        <h3>Overall user experience</h3>
        <h5>Vocal entry point</h5>
        <p>A user can enter the experience by saying ‚ÄúHey Google, open <span class="var">Protagonist</span>‚Äù. A minimal orchestral soundtrack signifies that the experience is beginning... ‚ÄùWelcome to Protagonist‚Äù, a hushed voice says. ‚ÄúWould you like to continue your last story or search for something new?‚Äù If the user wants to begin a new story, they vocally navigate a menu that sorts stories into genres and eventually pick one by saying the name of the story (see <a href="#menunav">menu navigation</a> for specifics). </p>
        <p>A user can also enter the experience by avoiding the menu, and selecting a specific known story. They might say ‚ÄúHey Google, go to Camp Nightmare‚Äù. If the user chooses a specific story, it begins immediately playing the introduction to the story, bypassing the normal product introduction.</p>
      </div>


      <div class="grid-1 has-animation image-next">
        <h5>App entry point</h5>
        <p>A user can also trigger a story through their Google Home app (Figure 1), by navigating to the browse tab and tapping on the story.</p>
      </div>

      <div class="has-animation">
        <img data-src="img/protagonist/entrypoint.jpg" alt="The trebuchet, Arduino setup, and digital interface" data-action="zoom" class="lazy big-image">
        <p class="caption">Fig 2. The app entrypoint, embedded in the current Google Home Android app.</p>
      </div>

      <!-- <div class="has-animation">
        <img style="margin-top:15px; width:100%" data-src="img/protagonist/start.jpg" alt="Experiential prototyping with kids" data-action="zoom" class="lazy big-image">
        <p class="caption">Fig 2. Triggering the entry point in context.</p>
      </div> -->

      <div class="grid-1 has-animation" id="ideation">
        <h5>Bulk of the experience</h5>
        <p>The bulk of the experience is the user listening to the story and responding to prompts. If the user is playing Goosebumps: Camp Nightmare, a reimagination of a Goosebumps classic, it might begin like...</p>
        <blockquote >
          <p>A schoolbus is shaking and churning as it barrels down a highway, the excited chatter and laughter of children hanging in the air like a warm cloud.</p>
          <p><span class="heavy">Narrator<br></span> ‚ÄúIt‚Äôs a cold Autumn morning, and you and your friends are on your way to a school camping trip...‚Äù</p>
          <p><span class="heavy">Dori, one of the characters<br></span> My cousin told me not to wander off into the woods while we‚Äôre out camping. She said there‚Äôs ghosts out there. Do you believe in ghosts?</p>
          <p><span class="heavy">User, responding with voice<br></span> Hmm, I don‚Äôt really believe in them.</p>
          <p><span class="heavy">Dori<br></span> What! My cousin says she‚Äôs seen them out there. If you look for long enough you can see their red eyes between the trees.</p>
        </blockquote>

        <p>In this scenario, the program is listening for an affirmative or negative response. But the response doesn‚Äôt have to be as binary as ‚Äúyes‚Äù or ‚Äúno‚Äù‚Äìnatural language processing picks out the general sentiment and keywords to classify the response given by the user, and then select the appropriate next piece of the story. For specifics on the algorithm that performs this function, jump to <a href="#algorithm">decision nodes</a>.</p>
      </div>

      <div class="grid-1 has-animation image-next">
        <h5 id="menunav">Menu navigation</h5>
        <p>In order to select a story in the audio-only interface, the user must be able to browse a potentially large database of content with only their voice. Upon starting, Protagonist asks if the user would like to resume a story or browse story genres. If the user says browse, then Protagonist lists off the first five available genres, like ‚ÄúAdventure‚Äù or ‚ÄúMystery‚Äù, and asks if the user would like to hear more genres, if they are available. The user can select a genre by saying its name. This minimizes cognitive load and follows best-practice guidelines on menu navigation in audio interfaces.</p>
      </div>


      <div class="has-animation">
        <img style="width:100%" data-src="img/protagonist/menu-navigation.jpg" alt="An initial set of experiment concepts." data-action="zoom" class="lazy big-image">
        <p class="caption">Fig 3. Navigating the audio-only menu in Protagonist.</p>
      </div>

      <div class="grid-1 has-animation ">
        <p>Upon selection of a genre, Protagonist lists off the first five stories available and asks if the user wants to select one, hear more about one, or list off the next 5 available stories.</p>
        <p>The user can select a story by saying its name, or can request to ‚Äúhear more about ____‚Äù for a brief audio description of what the story is about.</p>
      </div>

      <div class="grid-1 has-animation">
        <h5>Exiting the experience</h5>
        <p>A user can ask Google to pause, play, or quit a story. This could be as simple as saying ‚ÄúGoogle, play / pause / quit‚Äù, but we can also extend the natural language processing to map to these functions‚Äìa user might say ‚Äústop‚Äù, ‚Äúend‚Äù, or ‚Äúfinish‚Äù, all of which are semantically similar to ‚Äúquit‚Äù. When the user returns, the story will resume from where it left off.</p>
      </div>

      <div class="grid-1 has-animation image-next" >
        <h5>Context of use</h5>
        <p>Protagonist is designed to work in the home setting, with a Google smart speaker device located in the same room as the user. The experience can be similarly extended to a mobile-only experience, but we will focus first on the in-home experience since it is more socially acceptable to vocally engage with a story in a private setting.</p>
      </div>

      <div class="grid-1 has-animation" style="width:100%">
        <h2>Architecture of a story</h2>
      </div>

      <div class="has-animation">
        <img style="margin-top:15px; width:100%" data-src="img/protagonist/architecture.jpg" alt="Experiential prototyping with kids" data-action="zoom" class="lazy big-image">
        <p class="caption">Fig 4. Abstracted view of modules that make up a story.</p>
      </div>

      <div class="grid-1 has-animation ">
        <h5>Story</h5>
        <p>A story is the top-level wrapper for a complete narrative structure, which the user can navigate end-to-end. It is comprised primarily of content, prompts & responses, decision nodes, and branches. Figure 4 outlines the architecture of a story.</p>
      </div>

      <div class="grid-1 has-animation " >
        <h5>Content</h5>
        <p>Content is the bulk of the story, taking the form of .wav files that are supplied by the content creator. Content is static and non-interactive, and is simply played back to the user through the Google Home‚Äôs built-in speaker. Content is up to the content creator, but is likely to contain character dialogue, ambient sounds, and musical scores that help to create an immersive story.</p>
      </div>

      <div class="grid-1 has-animation " >
        <h5>Prompts</h5>
        <p>Once the story reaches a point at which the user can interact with the narrative, they are given a prompt. Prompts are questions that are posed to the user, typically by the story narrator or by another character in the story, and are immediately followed by an audio cue that informs the user that they can speak to enter an input (see <a href="#sound">sound design</a> section). Prompts are supplied by content creators, as they are specific to the narrative in which they exist. </p>
      </div>

      <div class="grid-1 has-animation " >
        <h5>Responses</h5>
        <p>Responses are user-provided voice inputs that occur directly after a prompt is given. These are the inputs that are processed via the text-to-speech -> NLU pipeline. They can take any form that the user chooses, which introduces potential problems in recognition and content analysis issues. For specific directions in error recovery and response analysis, see <a href="#recovery">error recovery</a>.</p>
      </div>

      <div class="grid-1 has-animation "  >
        <h5>Decision nodes</h5>
        <p>Decision nodes are the points at which a story diverges into two or more different directions, or ‚Äúbranches‚Äù. Which branch is taken depends on the output of the response analyzing algorithms. See <a href="#algorithm">decision nodes</a> for an in-depth explanation of the types of nodes and their algorithms.</p>
      </div>

      <div class="grid-1 has-animation image-next " >
        <h5>Branches</h5>
        <p>Branches are the different paths taken throughout a story. One decision node can lead to one or more separate branches.</p>
      </div>

      <div class="grid-1 has-animation " style="width:100%">
        <h2>Technology specifications</h2>
      </div>

      <div class="has-animation">
        <img style="margin-top:15px; width:100%" data-src="img/protagonist/techstack.png" alt="Experiential prototyping with kids" data-action="zoom" class="lazy big-image">
        <p class="caption">Fig 5. The tech stack for Protagonist.</p>
      </div>

      <div class="grid-1 has-animation " >
        <h5>Platform</h5>
        <p>This product will be developed for the Google Home and Google Hub devices, with entry points also available via a linked Android device. The mobile device entry point will serve as a visual navigation aid to help users learn about available content, while the interactive audio experience will take place within the Google Home.</p>
      </div>

      <div class="grid-1 has-animation " >
        <h5>API justification</h5>
        <p>The design necessitates three main technology components: first is a deep learning speech recognition algorithm that will be used to capture and transcribe user voice input into machine-readable text. We will use the Google Cloud Speech-to-Text REST API for speech recognition, with audio input from the Google Home built-in microphone.</p>
        <p>The second component is natural language processing, for which we will use the IBM Cloud Natural Language Understanding (NLU) API. Text inputs generated from the Speech-to-Text API will be passed to NLU for processing, particularly to analyze for keywords and sentiment of the user input.</p>
        <p>The third component is decision trees, which will form the basis for the branching nature of the choose-your-own-adventure narrative. While ‚Äúplaying‚Äù the audio experience, users will be navigating the branching structure, determined by the outputs of the NLU API for keywords, and sentiment, which are mapped to different branches of the narrative. You can read more about the specifics of the decisions in the <a href="#branch">algorithm section</a>.</p>
      </div>

      <div class="grid-1 has-animation " style="width:100%" id="algorithm">
        <h2>Decision nodes</h2>
      </div>

      <div class="has-animation">
        <img style="margin-top:15px; width:100%" data-src="img/protagonist/nodetypes.png" alt="Experiential prototyping with kids" data-action="zoom" class="lazy big-image">
        <p class="caption">Fig 6. The 4 different types of nodes, explained below.</p>
      </div>

      <div class="grid-1 has-animation " >
        <h5>Concept</h5>
        <p>Decision nodes are the most important components of the story: they respond to user voice input and help to move the story forward. There are multiple types of decision nodes: trivial nodes, pivotal nodes, and reversal nodes (see Figure 6).</p>
        <p>A <span class="keyword">trivial node</span> is a section of the story where branches converge on the same path. As the name indicates, the user‚Äôs selection is trivial; it could be a question like ‚Äúwhat color shirt do you want to put on?‚Äù These types of questions give the user a more personalized experience; it‚Äôs not necessary for every node to lead to a major plot pivot.</p>
        <p>A <span class="keyword">pivotal node</span> occurs when there is a major plot pivot. In this case, the user is prompted with character dialogue, and based on their response will significantly affect the story.</p>
        <p>A <span class="keyword">reversal node</span> occurs when the user is forced back to an earlier branch in the story. This is a common tactic in choose-your-own-adventure novels and can help the user experience more pieces of the story.</p>
        <p>Finally, an <span class="keyword">ending node</span> leads to one of the story‚Äôs endings; it may or may not be paired with a prompt.</p>
      </div>


      <div class="grid-1 has-animation image-next" id="branch">
        <h5>Algorithm</h5>
        <p>Selecting a branch at a decision node is the key algorithmic piece of the Protagonist experience. Here‚Äôs how it works:</p>
        <ol>
          <li>Each branch option is assigned a set of sentiment and keyword tags. These could be terms like ‚Äúsad‚Äù, ‚Äúangry‚Äù, or ‚Äúselfish‚Äù.</li>
          <li>The user is given a prompt from a character in the story, and responds to the prompt with their voice.</li>
          <li>The Speech-to-Text REST API transcribes the voice input to text.</li>
          <li>The NLU API analyzes the text for keywords and sentiment, and generates corresponding tags for the user‚Äôs input.</li>
          <li>The tags from the user input are compared to the tags from the branch. In the case of a direct match, the matching branch is immediately selected. If there is no direct match, then the closest matching branch is selected.</li>
        </ol>
      </div>

      <div class="has-animation">
        <img style="margin-top:15px; width:100%" data-src="img/protagonist/semantic.png" alt="Experiential prototyping with kids" data-action="zoom" class="lazy big-image">
        <p class="caption">Fig 7. Process of mapping tags to branches.</p>
      </div>


      <div class="grid-1 has-animation" id="recovery">
        <h2>Error Prevention & Recovery</h2>
        <p>User input analysis has the greatest potential for user frustration, since an erroneous choice of branch could break immersion and make the user feel as if they aren‚Äôt in control of the story. To minimize errors here, we will use the following method to analyze a user response (also illustrated above in figure 7):</p>
        <ol>
          <li>Using the IBM NLU API, generate at least 5 novel tags based on the voice input. Tags with similar semantic meaning (e.g. ‚Äúsad‚Äù or ‚Äúgrieving‚Äù) should be grouped and counted as a single tag to allow for greater diversity in the list of tags. The API also outputs its confidence in the assigned tag, ranging from 0 to 1.0.</li>
          <li>Compare the generated tags with the tags assigned to each branch, and calculate the similarity of their outputs. For example, if the API outputs a ‚Äúsad‚Äù sentiment with .67 confidence, and an ‚Äúangry‚Äù sentiment with .29 confidence, then the ‚Äúsad‚Äù sentiment should carry greater statistical weight. So if one of the branches is assigned the tag ‚Äúunhappy‚Äù, and the other is assigned ‚Äúsatisfied‚Äù, then the user‚Äôs input will trigger the branch with the ‚Äúunhappy‚Äù tag.</li>
          <li>If the generated tags all have low confidence (< .2), then it could signify an issue with the microphone, an issue with the user‚Äôs input, or failure in translating user input into text. The next section addresses these issues.</li>
        </ol>

      </div>

      <div class="grid-1 has-animation" >
        <h5>Recovery from poor vocal input quality or inaccurate voice-to-text</h5>
        <p>A known limitation of speech recognition algorithms is their relatively high levels of inaccuracy. This may result in unrecognized user input which is unable to be analyzed for sentiment and keywords.</p>
        <p>If the classification of user input is impossible, a clarifying question can be used to ask the user for clarification. For example, if Dori asks the user if they believe in ghosts, and the user‚Äôs response is unrecognizable, a .wav file can be triggered so that Dori asks ‚ÄúWhat? I don‚Äôt know what you said‚Äù, or whatever the content creator chooses. If after two clarifying questions it is still impossible to assign tags to the input, a branch is randomly chosen.</p>
      </div>

      <div class="grid-1 has-animation" >
        <h5>Minimizing noisy inputs</h5>
        <p>To minimize interference between speaker output and user voice input, the output volume should be reduced to 10% of its current volume while the user is giving their input. Though it could potentially break immersion, this is common practice with smart speaker setups so that voice input is not obscured by the Google Home‚Äôs sound output.</p>
      </div>

      <div class="grid-1 has-animation" id="sound" >
        <h2>Sound design</h2>
        <p>Sound design is integral to the experience of Protagonist. However, a majority of the audio is provided as .wav files by content creators. There is only one sound effect that persists through all stories on the platform: the prompt cue. Whenever the user reaches a prompt, there is a brief audio cue directly following the end of the prompt to let the user know that the story is awaiting vocal input.</p>
      </div>

      <div class="has-animation">
        <img style="margin-top:15px; width:100%" data-src="img/protagonist/sound.png" alt="Experiential prototyping with kids" data-action="zoom" class="lazy big-image">
        <p class="caption">Fig 8. Layering of sounds during user response window</p>
      </div>

      <div class="grid-1 has-animation" >
        <p>Sound design becomes more complex when the user is expected to give a response (see Figure 8). During the response window, there is either a background musical score or a soundscape, depending on whichever is used by the content creator. This is included to avoid breaking the flow of the story. If the user does not respond for 5-8 seconds, a re-prompt may be triggered: this is when the character that posed the prompt can prompt the user again, in case they don‚Äôt remember the question or are unable to answer. This can be either the same prompt, or a rephrasing of the prompt (e.g. ‚Äúso do you believe in ghosts or what?‚Äù These must be supplied as additional .wav files that are triggered on a regular cadence.</p>
        <p>After 25 seconds with no input from the user, a branch is randomly chosen.</p>
      </div>



      <div class="grid-1 has-animation " >
        <h2>Visual design</h2>
        <p>There is no additional visual UI component to Protagonist, as it lives in the Google Home App. The only UI customization will be a hero image that is uploaded by a content creator. This hero image appears in Figure 1 and 2. The only brand asset needed is the Protagonist logo, below.</p>
        <img style="margin-top:15px; width:100%" data-src="img/protagonist/logo.png" alt="Experiential prototyping with kids" data-action="zoom" class="lazy big-image">
      </div>

      <center><div id="divider-dotted" class="divider"></div></center>


      <div class="grid-1 has-animation" >
        <p>Woah, did you really read all of that? Yikes. Take a <a href="files/protagonist.pdf">downloadable PDF version</a> of this so you can read again at your leisure, ya freak.</p>
      </div>

      <div class="grid-1" style="align-items:center; justify-content: center; text-align:center">
        <p style="font-size:32px;"><a href="library.html">NEXT PROJECT</a></p>
      </div>













    </div>
  </div>











    <div class="page-content">
      <div class="container">
        <div class="global-footer" id="copyright">
          <div>
            <p><i class="fas fa-paper-plane"></i> &nbsp;<a href="/files/EngelOliver_Resume.pdf" target="_blank">Resume</a></p>
            <p><i class="fab fa-linkedin"></i> &nbsp;<a href="https://www.linkedin.com/in/oengel/" target="_blank">LinkedIn</a></p>
          </div>
          <div style="text-align:right">
            <img id="logo-gif" src="img/logo.gif" style="background:white;">
            <p>Updated <span class="last-updated"></span></p>
          </div>
        </div>
      </div>
    </div>

    <script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
    <script src="js/oliver.js"></script>
    <script src="js/slideshow.js"></script>
    <script src="js/rellax.min.js"></script>
    <script>
      // Accepts any class name
      var rellax = new Rellax('.rellax');
    </script>
    <link href="css/zoom.css" rel="stylesheet">
    <script src="js/zoom.min.js"></script>
    <script src="js/transition.min.js"></script>


</body>

</html>
